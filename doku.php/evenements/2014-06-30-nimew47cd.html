<!DOCTYPE html>
<html lang="fr" dir="ltr">

<!-- Mirrored from notation.afim-asso.org/doku.php/evenements/2014-06-30-nimew?do=export_xhtml by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 19 Oct 2020 07:56:30 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
  <meta charset="utf-8" />
  <title>evenements:2014-06-30-nimew</title>
<meta name="generator" content="DokuWiki"/>
<meta name="robots" content="index,follow"/>
<meta name="keywords" content="evenements,2014-06-30-nimew"/>
<link rel="start" href="../../index.html"/>
<link rel="manifest" href="../../lib/exe/manifest.php"/>
<link rel="alternate" type="application/rss+xml" title="Derniers changements" href="../../feed.php"/>
<link rel="alternate" type="application/rss+xml" title="Catégorie courante" href="../../feedf001.php?mode=list&amp;ns=evenements"/>
<link rel="alternate" type="text/html" title="HTML brut" href="2014-06-30-nimew47cd.html?do=export_xhtml"/>
<link rel="canonical" href="2014-06-30-nimew.html"/>
<link rel="stylesheet" type="text/css" href="../../lib/exe/cssc55d.css?t=A_Centered_Perspective&amp;tseed=22a5003533456f171a7552a81b2eda2d"/>
<!--[if gte IE 9]><!-->
<script type="text/javascript">/*<![CDATA[*/var NS='evenements';var JSINFO = {"id":"evenements:2014-06-30-nimew","namespace":"evenements","ACT":"export_xhtml","useHeadingNavigation":0,"useHeadingContent":0};
/*!]]>*/</script>
<script type="text/javascript" charset="utf-8" src="../../lib/exe/jquery1d4f.php?tseed=23f888679b4f1dc26eef34902aca964f"></script>
<script type="text/javascript" charset="utf-8" src="../../lib/exe/jsc55d.php?t=A_Centered_Perspective&amp;tseed=22a5003533456f171a7552a81b2eda2d"></script>
<!--<![endif]-->
</head>
<body>
<div class="dokuwiki export">

<p>
<a href="../../lib/exe/detail.php/evenements/2014-06-30-workshop/dscf6424c5ca.html?id=evenements%3A2014-06-30-nimew" class="media" title="evenements:2014-06-30-workshop:dscf6424.jpg"><img src="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/dscf642437d2.jpg_%3b?w=200&amp;tok=f11f67" class="mediaright" alt="" width="200" /></a>
</p>

<h2 class="sectionedit1" id="interactive-music-notation-and-representation-workshop">Interactive Music Notation and Representation Workshop</h2>
<div class="level2">

<p>
<strong>June 30 2014 - 9:30 to 13:00 <br/>

Goldsmiths, University of London, London, UK</strong>
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Interactive Music Notation and Representation Workshop&quot;,&quot;hid&quot;:&quot;interactive-music-notation-and-representation-workshop&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:1,&quot;range&quot;:&quot;55-204&quot;} -->
<h3 class="sectionedit2" id="nime-2014">@NIME 2014</h3>
<div class="level3">

<p>
<a href="http://www.nime2014.org/" class="urlextern" title="http://www.nime2014.org" rel="nofollow">www.nime2014.org</a>
</p>

<p>
<a href="nimewcfp.html" class="wikilink1" title="evenements:nimewcfp">Call for participation</a>
</p>

<p>
Computer music tools for music notation have long been restricted to conventional approaches and dominated by a few systems, mainly oriented towards music engraving. During the last decade and driven by artistic and technological evolutions, new tools and new forms of music representation have emerged. The recent advent of systems like Bach, MaxScore or INScore (to cite just a few), clearly indicates that computer music notation tools have become mature enough to diverge from traditional approaches and to explore new domains and usages such as interactive and live notation.
</p>

<p>
The aim of the workshop is to gather artists, researchers and application developers, to compare the views and the needs inspired by contemporary practices, with a specific focus on interactive and live music, including representational forms emerging from live coding. Special consideration will be given to new instrumental forms emerging from the NIME community.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;@NIME 2014&quot;,&quot;hid&quot;:&quot;nime-2014&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:2,&quot;range&quot;:&quot;205-1243&quot;} -->
<h4 class="sectionedit3" id="registration">Registration</h4>
<div class="level4">

<p>
Workshop attendees must register for NIME for at least one of Tuesday,
Wednesday or Thursday (see <a href="http://www.nime2014.org/practical-info/registration/" class="urlextern" title="http://www.nime2014.org/practical-info/registration/" rel="nofollow">here</a>) but don&#039;t pay for the workshop day. 
You must send an email to <a href="mailto:&#x64;&#x66;&#x6f;&#x62;&#x65;&#x72;&#x40;&#x67;&#x6d;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;" class="mail" title="&#x64;&#x66;&#x6f;&#x62;&#x65;&#x72;&#x40;&#x67;&#x6d;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;">&#x64;&#x66;&#x6f;&#x62;&#x65;&#x72;&#x40;&#x67;&#x6d;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;</a> with your coordinates to be registered to the workshop itself.
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Registration&quot;,&quot;hid&quot;:&quot;registration&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:3,&quot;range&quot;:&quot;1244-1572&quot;} -->
<h4 class="sectionedit4" id="program">Program</h4>
<div class="level4">
<ul>
<li class="level1"><div class="li"> 9h30 </div>
<ul>
<li class="level2"><div class="li"> <strong>Introduction</strong></div>
</li>
<li class="level2"><div class="li"> <strong>Animated Notation Dot Com: 2014 Report</strong> <br/>
Ryan Ross Smith </div>
</li>
<li class="level2"><div class="li"> <strong>Timelines in Algorithmic Notation</strong> <br/>
Thor Magnusson</div>
</li>
<li class="level2"><div class="li"> <strong>Breaking the Notational Barrier: Liveness in Computer Music </strong> <br/>
Chris Nash</div>
</li>
</ul>
</li>
<li class="level1"><div class="li"> 10h30</div>
<ul>
<li class="level2"><div class="li"> <strong>Quid Sit Musicus: Interacting with Calligraphic Gestures</strong> <br/>
Jérémie Garcia, Gilbert Nouno, Philippe Leroux</div>
</li>
<li class="level2"><div class="li"> <strong>Non-Visual Scores for Ensemble Comprovisation</strong> <br/>
Sandeep Bhagwati</div>
</li>
<li class="level2"><div class="li"> <strong>Interactive and real-time composition with soloists and music ensembles</strong> <br/>
Georg Hajdu </div>
</li>
<li class="level2"><div class="li"> <strong>A javascript library for collaborative composition of leadsheets</strong> <br/>
Daniel Martín, François Pachet</div>
</li>
<li class="level2"><div class="li"> <strong>(Pre)compositional strategies and computer-generated notation in surface/tension (2012) for oboe and piano or ensemble</strong> <br/>
Sam Hayden</div>
</li>
<li class="level2"><div class="li"> <strong>On- and off-screen: presentation and notation in interactive electronic music</strong> <br/>
Pete Furniss</div>
</li>
<li class="level2"><div class="li"> <strong>John Cage Solo for Sliding Trombone, a Computer Assisted Performance approach</strong> <br/>
Benny Sluchin, Mikhaïl Malt</div>
</li>
<li class="level2"><div class="li"> <strong>Deriving a Chart-Organised Notation from a Sonogram Based Exploration: TIAALS (Tools for Interactive Aural Analysis)</strong> <br/>
Michael Clarke, Frédéric Dufeu, Peter Manning</div>
</li>
</ul>
</li>
<li class="level1"><div class="li"> 12h30 </div>
<ul>
<li class="level2"><div class="li"> <strong>Discussion</strong></div>
</li>
</ul>
</li>
<li class="level1"><div class="li"> 13h00 - end</div>
</li>
</ul>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Program&quot;,&quot;hid&quot;:&quot;program&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:4,&quot;range&quot;:&quot;1573-2846&quot;} -->
<h4 class="sectionedit5" id="organisation">Organisation</h4>
<div class="level4">
<ul>
<li class="level1"><div class="li"> Dominique Fober         - Grame - Lyon</div>
</li>
<li class="level1"><div class="li"> Jean Bresson               - Ircam - Paris</div>
</li>
<li class="level1"><div class="li"> Pierre Couprie             - IReMus, Université Paris-Sorbonne - Paris</div>
</li>
<li class="level1"><div class="li"> Yann Geslin                 - INA/GRM - Paris</div>
</li>
<li class="level1"><div class="li"> Richard Hoadley         - Anglia Ruskin University - Cambridge</div>
</li>
</ul>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Organisation&quot;,&quot;hid&quot;:&quot;organisation&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:5,&quot;range&quot;:&quot;2847-3151&quot;} -->
<h4 class="sectionedit6" id="contact">Contact</h4>
<div class="level4">

<p>
Please send enquiries to <a href="mailto:&#x64;&#x66;&#x6f;&#x62;&#x65;&#x72;&#x40;&#x67;&#x6d;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;" class="mail" title="&#x64;&#x66;&#x6f;&#x62;&#x65;&#x72;&#x40;&#x67;&#x6d;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;">&#x64;&#x66;&#x6f;&#x62;&#x65;&#x72;&#x40;&#x67;&#x6d;&#x61;&#x69;&#x6c;&#x2e;&#x63;&#x6f;&#x6d;</a>
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Contact&quot;,&quot;hid&quot;:&quot;contact&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:6,&quot;range&quot;:&quot;3152-3213&quot;} -->
<h4 class="sectionedit7" id="detailed-program">Detailed program</h4>
<div class="level4">

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Detailed program&quot;,&quot;hid&quot;:&quot;detailed-program&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:7,&quot;range&quot;:&quot;3214-3239&quot;} -->
<h5 class="sectionedit8" id="animated-notation-dot-com2014-report">Animated Notation Dot Com: 2014 Report</h5>
<div class="level5">

<p>
<strong>Ryan Ross Smith</strong> <em>Rensselaer Polytechnic Institute</em>
</p>

<p>
<a href=http://animatednotation.com/>Animatednotation.com</a> [ANDC] was created in order to document the emerging field of animated notation
[an umbrella term encompassing real-time/spontaneous/moving/generative/etc./music notation], as a 
platform to encourage discourse, and to place this field within a broader historical context. This brief 
talk will cover what precedents motivated the creation of ANDC, and report on its current state and 
future leanings.
</p>

<p>
Link:
</p>
<ul>
<li class="level1"><div class="li"> <a href="http://animatednotation.com/" class="urlextern" title="http://animatednotation.com/" rel="nofollow">http://animatednotation.com/</a> <br/>
</div>
</li>
<li class="level1"><div class="li"> <a href="http://animatednotation.com/RyanRossSmith.html" class="urlextern" title="http://animatednotation.com/RyanRossSmith.html" rel="nofollow">http://animatednotation.com/RyanRossSmith.html</a></div>
</li>
<li class="level1"><div class="li"> <a href="https://www.youtube.com/watch?v=l11U25bL3Qs" class="urlextern" title="https://www.youtube.com/watch?v=l11U25bL3Qs" rel="nofollow">Video</a></div>
</li>
</ul>
<hr />

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Animated Notation Dot Com: 2014 Report&quot;,&quot;hid&quot;:&quot;animated-notation-dot-com2014-report&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:8,&quot;range&quot;:&quot;3240-3979&quot;} -->
<h5 class="sectionedit9" id="timelines-in-algorithmic-notation">Timelines in Algorithmic Notation</h5>
<div class="level5">

<p>
<strong>Thor Magnusson</strong> <em>University of Sussex</em>
</p>

<p>
Computer code is a form of notational language. It prescribes actions to be carried out by the computer, often by systems called interpreters. When code is used to write music, we are therefore operating with programming language as a relatively new form of musical notation. Here code and traditional notation are somewhat at odds, since code is written as text, without any representational timeline. This can pose problems, for example for a composer who is working on a section in the middle of a long piece, but has to repeatedly run the code from the beginning or make temporary arrangements to solve this difficulty in the compositional process. In short: code does not come with a timeline but is rather the material used for building timelines. This article explores the context of creating linear “code scores” in the area of musical notation. 
</p>

<p>
Link:
</p>
<ul>
<li class="level1"><div class="li"> <a href="https://www.youtube.com/watch?v=sJQvwFIrSE0" class="urlextern" title="https://www.youtube.com/watch?v=sJQvwFIrSE0" rel="nofollow">Video</a></div>
</li>
</ul>
<hr />

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Timelines in Algorithmic Notation&quot;,&quot;hid&quot;:&quot;timelines-in-algorithmic-notation&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:9,&quot;range&quot;:&quot;3980-4997&quot;} -->
<h5 class="sectionedit10" id="breaking-the-notational-barrierliveness-in-computer-music">Breaking the Notational Barrier: Liveness in Computer Music</h5>
<div class="level5">

<p>
<strong>Chris Nash</strong> <em>Centre for Music and Science (<abbr title="Content Management System">CMS</abbr>), 
Faculty of Music, University of Cambridge</em>
</p>

<p>
There are many roles for notation in music. In one instance, notation provides an asynchronous, persistent, and formal mode of unidirectional communication between artists, performers, teachers, and scholars. But in the creative process, lo-fidelity forms of notation (e.g. sketching) provide a more informal, more interactive tool for composers to explore and experiment with new musical ideas. Accordingly with the advent of interactive computer music systems, a disjunction has arisen between these roles and what we expect of digital notations and UIs.
This talk focuses on the challenges and opportunities within digital music systems for supporting liveness and flow in notation-mediated interaction. Using real-world examples and findings from extensive user studies, the central role of rapid, incremental feedback from the domain (e.g. sound, music) is emphasised. The benefit of feedback is specifically demonstrated through the example of tracker-style sequencers, where interaction with a text-based musical notation is driven by rapid edit-audition cycles that enable flow and a high level of liveness and engagement during composition. The Cognitive Dimensions of Notations framework (Green and Petre, 1996) is discussed in a musical context, and a new framework based on modelling the feedback loops within an interactive system (Nash, 2014) is introduced to highlight common paradigms in modern music tools, such as sequencers and DAWs, discussing notation in the context of theories of liveness (Tanimoto, 1990) and flow (Csikszentmihalyi, 1996).
</p>

<p>
Link:
</p>
<ul>
<li class="level1"><div class="li"> <a href="https://www.youtube.com/watch?v=ke3aHGh-FSo" class="urlextern" title="https://www.youtube.com/watch?v=ke3aHGh-FSo" rel="nofollow">Video</a></div>
</li>
<li class="level1"><div class="li"> <a href="../../lib/exe/fetch.php/evenements/nime_2014_workshop.zip_%3b" class="media mediafile mf_zip" title="evenements:nime_2014_workshop.zip (24.6 MB)">Slides</a></div>
</li>
</ul>
<hr />

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Breaking the Notational Barrier: Liveness in Computer Music&quot;,&quot;hid&quot;:&quot;breaking-the-notational-barrierliveness-in-computer-music&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:10,&quot;range&quot;:&quot;4998-6848&quot;} -->
<h5 class="sectionedit11" id="quid-sit-musicusinteracting-with-calligraphic-gestures">Quid Sit Musicus: Interacting with Calligraphic Gestures</h5>
<div class="level5">

<p>
<strong>Jeremie Garcia, Gilbert Nouno, Philippe Leroux</strong> <em>INRIA, UPSud, CNRS (LRI) - STMS Lab: IRCAM-CNRS-UPMC - McGill University &amp; CIRMMT</em>
</p>

<p>
<a href="../../lib/exe/detail.php/evenements/2014-06-30-workshop/garciac5ca.html?id=evenements%3A2014-06-30-nimew" class="media" title="evenements:2014-06-30-workshop:garcia.png"><img src="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/garciac147.png_%3b?w=200&amp;tok=5a6173" class="mediaright" alt="" width="200" /></a>Illuminated manuscripts of medieval music contain rich decorations in addition to handwritten neumatic notation. Our project investigated the use of such handwritten symbols for the composition of the piece <em>Quid sit musicus</em> by Philippe Leroux. We designed several tools that combine computer-aided composition with interactive paper to support the composition process as well as the performance of the piece.
Interactive paper creates new opportunities to interact with handwritten elements and explore computer based musical processes. In this workshop, we will present how the composer used the calligraphic notation as compositional material and demonstrate the tools we developed for this project.<br/>

</p>

<p>
The interactive paper interface captures the handwritten gestures of the composer over the original manuscript and extracts both graphical and dynamic features before transmitting the data to external computer-aided composition environments. We expanded the composer’s library in OpenMusic and used a reactive extension of the software to create patches that respond to the pen interactions. The composer used this environment to design several processes to create the harmony of the piece, rhythms, melodies and harmonic gestures from the pen data with direct audio-visual feedback. By clicking over an existing form with the pen, the composer can select and explore the features of previously drawn shapes.
We also created patches in Max to synthesize sounds and control the spatialization from the shapes’ data such as their perimeters. We will use these patches for the performance of the piece that will be premiered in June at Ircam.
During this project, Philippe Leroux defined new meanings for the neumatic musical notation of the medieval manuscript. The interactive paper interface combined with computer-based composition environments allowed the composer to use handwritten symbols as gestures with dynamic properties and as interactive elements to trigger, control and explore musical processes.
</p>

<p>
Link:
</p>
<ul>
<li class="level1"><div class="li"> <a href="https://www.youtube.com/watch?v=5TwEe68OAnU" class="urlextern" title="https://www.youtube.com/watch?v=5TwEe68OAnU" rel="nofollow">Video</a></div>
</li>
<li class="level1"><div class="li"> <a href="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/workshop_notation_nime_garcia_low.pdf_%3b" class="media mediafile mf_pdf" title="evenements:2014-06-30-workshop:workshop_notation_nime_garcia_low.pdf (1.9 MB)">Slides</a></div>
</li>
</ul>
<hr />

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Quid Sit Musicus: Interacting with Calligraphic Gestures&quot;,&quot;hid&quot;:&quot;quid-sit-musicusinteracting-with-calligraphic-gestures&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:11,&quot;range&quot;:&quot;6849-9281&quot;} -->
<h5 class="sectionedit12" id="non-visual-scores-for-ensemble-comprovisation">Non-Visual Scores for Ensemble Comprovisation</h5>
<div class="level5">

<p>
<strong>Sandeep Bhagwati</strong> <em>Matralab - Montréal</em>
</p>

<p>
A visual interface is not always the most ideal method of conveying information to a musician – it may well be an unwelcome distraction. Many soloists in classical music learn their part by heart in order to fully concentrate on interpretation. Having to look at an interactive score interface may often occupy more of the player&#039;s attention than the actual shaping of sound or musical architecture. Indeed, in performances of interactive score-based music, the performer is visibly much more “glued” to the visual score interface than in both traditional classical composed music and, obviously, in improvisation.
The need for non-visual scores becomes obvious when we consider an ensemble of moving performers or non-standard, reactive or reconfigurable spatial arrangements of musicians (as in several works by the author). After extensive (and still ongoing) exploration of interactive on-screen scores, the author has, in recent and future research-creation projects decided to explore the artistic viability of non-visual interaction with musicians: either with purely aural scores or via a body suit score interface, to be developed with a major 4 year research-creation grant recently awarded by the Canadian Social Sciences and Humanities Research Council. The talk will describe the intellectual and technical context of non-visual scores, the tools and the current state of thinking about different approaches to live-scoring music (one of which is his comprehensive definition of comprovisation) and will give an insight into a collaboroative, emerging research-creation project, inviting listeners to contribute to this aspect of interactive music notation.
</p>

<p>
Link:
</p>
<ul>
<li class="level1"><div class="li"> <a href="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/bhagwati_nime.pdf_%3b" class="media mediafile mf_pdf" title="evenements:2014-06-30-workshop:bhagwati_nime.pdf (19.9 MB)">Slides</a></div>
</li>
<li class="level1"><div class="li"> <a href="https://www.youtube.com/watch?v=kKoOUfSUjNc" class="urlextern" title="https://www.youtube.com/watch?v=kKoOUfSUjNc" rel="nofollow">Video</a></div>
</li>
</ul>
<hr />

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Non-Visual Scores for Ensemble Comprovisation&quot;,&quot;hid&quot;:&quot;non-visual-scores-for-ensemble-comprovisation&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:12,&quot;range&quot;:&quot;9282-11191&quot;} -->
<h5 class="sectionedit13" id="interactive-and-real-time-composition-with-soloists-and-music-ensembles">Interactive and real-time composition with soloists and music ensembles</h5>
<div class="level5">

<p>
<strong>Georg Hajdu</strong>
</p>

<p>
In “Extreme Sight-reading, Mediated Expression, and Audience Participation: Real-time Music Notation In Live Performance” Jason Freeman described how real-time composition can be taken to new extremes: composers “waiting to create the score until during the performance”. In my presentation I shall describe several scenarios involving real-time composition, arrangement and/or part extraction to which classically trained musicians have been exposed to. For this, I have integrated MaxScore (developed by Nick Didkovsky and myself) into my networked multimedia performance environment Quintet.net allowing scores and parts to be delivered over the network and displayed on individual computer screens or tablets. 
</p>

<p>
The compositions in question are three pieces of mine: Ivresse &#039;84 (2007) for violinist and 4 laptop performers, schwer…unheimlich schwer (2009/11) for bass clarinet, viola, piano and percussion, and Swan Song (2011/12) for cello, percussion and multimedia. In contrast to the first two pieces, Swan Song uses a fixed score to control all aspects of the performance (musicians, audio and video); nonetheless, MaxScore&#039;s real-time capabilities were a boon during the rehearsals as changes could be made on the fly without the need to print new parts on paper, and annotations and suggestions could be fixed for future reference.
</p>

<p>
Furthermore, I shall focus on the current state of the art of real-time composition in man-machine networks as well on future developments affording participating musicians more control over the composition and editing process.
</p>
<hr />

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Interactive and real-time composition with soloists and music ensembles&quot;,&quot;hid&quot;:&quot;interactive-and-real-time-composition-with-soloists-and-music-ensembles&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:13,&quot;range&quot;:&quot;11192-12868&quot;} -->
<h5 class="sectionedit14" id="a-javascript-library-for-collaborative-composition-of-leadsheets">A javascript library for collaborative composition of leadsheets</h5>
<div class="level5">

<p>
<strong>Daniel Martín, François Pachet</strong> <em>Sony CSL - Paris</em>
</p>

<p>
<a href="../../lib/exe/detail.php/evenements/2014-06-30-workshop/martinc5ca.html?id=evenements%3A2014-06-30-nimew" class="media" title="evenements:2014-06-30-workshop:martin.png"><img src="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/martine522.png_%3b?w=200&amp;tok=ed1994" class="mediaright" alt="" width="200" /></a>A leadsheet is score composed by a melody (most of the times monophonic) and a grid of chord labels. Leadsheets are widely used to represent popular music songs as found in jazz and bossa nova song books.
Existing score editors can be divided into desktop­based for individuals (e.g. Sibelius and Finale), and web­based social oriented ones like NoteFlight (<a href="http://www.noteflight.com/" class="urlextern" title="http://www.noteflight.com" rel="nofollow">www.noteflight.com</a>) which is a social network in which users can create and share scores. NoteFlight has a web friendly score editor with which users can comment and recommend scores, and edit them when they have the permission. Chromatik is a social network to share scores, where scores can be uploaded in PDF format and users can make annotations in it. However, these systems cannot be easily embedded in other applications or easily extended. Furthermore, the considered systems, do not provide annotation tools for commenting on specific parts of the score.
</p>

<p>
The Leadsheet Editor (LE) enables users to edit and play leadsheets intuitively. The LE provides an interface to edit melodies and chord progressions, as well as a Json format for representing and storing leadsheets.
Users can make suggestions of modifications on the most critical parts of leadsheets, such as notes, chords, bars, chord transitions and structure (see figure). The LE is implemented in javascript and can be easily embedded in any web­based application, such as MusicCircles, developed in the Praise project. It has also been used in the FlowMachines project to enter over 10,000 jazz and Bossa nova songs. The modifications made by users are stored as suggestions that the original author can later visualize and apply or discard. They can be accompanied by text or audio examples. The LE also proposes features to edit a lead sheet collaboratively.
</p>

<p>
Links:
</p>
<ul>
<li class="level1"><div class="li"> <a href="http://xn--flow-machines-fsa.com/leadsheet_editor" class="urlextern" title="http://flow-­machines.com/leadsheet_editor" rel="nofollow">http://flow-­machines.com/leadsheet_editor</a> </div>
</li>
<li class="level1"><div class="li"> <a href="http://www.iiia.csic.es/praise/" class="urlextern" title="http://www.iiia.csic.es/praise/" rel="nofollow">http://www.iiia.csic.es/praise/</a></div>
</li>
<li class="level1"><div class="li"> <a href="https://www.youtube.com/watch?v=yheP-2gswCw" class="urlextern" title="https://www.youtube.com/watch?v=yheP-2gswCw" rel="nofollow">Video</a></div>
</li>
<li class="level1"><div class="li"> <a href="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/dmartin-javascript-lib.pdf_%3b" class="media mediafile mf_pdf" title="evenements:2014-06-30-workshop:dmartin-javascript-lib.pdf (1.7 MB)">Slides</a></div>
</li>
</ul>
<hr />

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;A javascript library for collaborative composition of leadsheets&quot;,&quot;hid&quot;:&quot;a-javascript-library-for-collaborative-composition-of-leadsheets&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:14,&quot;range&quot;:&quot;12869-15125&quot;} -->
<h5 class="sectionedit15" id="pre-compositional-strategies-and-computer-generated-notation-in-surfacetension-2012-for-oboe-and-piano-or-ensemble">(Pre)compositional strategies and computer-generated notation in surface/tension (2012) for oboe and piano or ensemble</h5>
<div class="level5">

<p>
<strong>Sam Hayden</strong> <em>Trinity Laban Conservatoire of Music and Dance</em>
</p>

<p>
My recent cycle of acoustic works, misguided (2011) for ELISION Ensemble, surface/tension (2012) for oboe and piano/ensemble, composed in collaboration with Christopher Redgate, as part of his AHRC research project ‘New Music for a New Oboe’, and a new String Quartet (2013-14) composed for Quatuor Diotima, has involved different solutions to the same broad initial context, using computer-assisted compositional techniques (CAC) via OpenMusic to help solve the problem of how to enable the proliferation of diverse surface materials, whilst maintaining an underlying formal coherence. In the case of surface/tension, the composition and notation of entirely acoustic music is inseparable from CAC tools and purpose-built acoustic instruments. surface/tension evolved from a dialectical relationship between the unique sonic possibilities inherent in the new Redgate-Howarth oboe and CAC techniques inherent in my (pre)compositional strategies, prototypes (OM patches) which were used to compose misguided. The underlying material for surface/tension was the product of two distinct CAC techniques, each yielding a different kind of ‘found object’ which became the starting points for the piece: (a) the spectral analyses of multiphonics, used to generate inharmonic non-tempered microtonal pitch fields (using Audiosculpt and OM); and (b) the algorithmic generation of artificial spectra (equal-tempered microtonal non-octavating scales) and complex structures of embedded rhythmical subdivisions. The collaborative process and the formalized (pre)compositional strategies, shaped directly both the notation of hyper-virtuosic material and the approach to form, taking the piece in unanticipated directions. Towards the latter stages of surface/tension, OM was used to interpolate between the artificial OM-generated spectra and the pitch fields derived from multiphonics, related concepts that became central to the composition of the String Quartet. The use of such digitally mediated notational tools has helped to develop a specific conception of material, aiding the creation of new musical ideas, sounds and modes of expression, beyond existing paradigms.
</p>

<p>
Links:
</p>
<ul>
<li class="level1"><div class="li"> Personal website: <a href="http://samhaydencomposer.com/" class="urlextern" title="http://samhaydencomposer.com/" rel="nofollow">http://samhaydencomposer.com/</a></div>
</li>
<li class="level1"><div class="li"> Trinity Laban profile: <a href="http://www.trinitylaban.ac.uk/students-staff/staff-biographies/sam-hayden" class="urlextern" title="http://www.trinitylaban.ac.uk/students-staff/staff-biographies/sam-hayden" rel="nofollow">http://www.trinitylaban.ac.uk/students-staff/staff-biographies/sam-hayden</a> </div>
</li>
<li class="level1"><div class="li"> Hayden portrait CD (NMC Debut Discs): <a href="http://www.nmcrec.co.uk/recording/presenceabsence" class="urlextern" title="http://www.nmcrec.co.uk/recording/presenceabsence" rel="nofollow">http://www.nmcrec.co.uk/recording/presenceabsence</a></div>
</li>
<li class="level1"><div class="li"> Christopher Redgate (21st Century Oboe Project): <a href="http://21stcenturyoboe.com/Sam-Hayden.php" class="urlextern" title="http://21stcenturyoboe.com/Sam-Hayden.php" rel="nofollow">http://21stcenturyoboe.com/Sam-Hayden.php</a> </div>
</li>
<li class="level1"><div class="li"> HCMF performance: <a href="http://www.hcmf.co.uk/event/show/285" class="urlextern" title="http://www.hcmf.co.uk/event/show/285" rel="nofollow">http://www.hcmf.co.uk/event/show/285</a></div>
</li>
<li class="level1"><div class="li"> <a href="https://www.youtube.com/watch?v=H6797dApylc" class="urlextern" title="https://www.youtube.com/watch?v=H6797dApylc" rel="nofollow">Video</a></div>
</li>
<li class="level1"><div class="li"> <a href="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/hayden_nime_presentation.pdf_%3b" class="media mediafile mf_pdf" title="evenements:2014-06-30-workshop:hayden_nime_presentation.pdf (3.1 MB)">Slides</a></div>
</li>
</ul>
<hr />

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;(Pre)compositional strategies and computer-generated notation in surface\/tension (2012) for oboe and piano or ensemble&quot;,&quot;hid&quot;:&quot;pre-compositional-strategies-and-computer-generated-notation-in-surfacetension-2012-for-oboe-and-piano-or-ensemble&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:15,&quot;range&quot;:&quot;15126-18040&quot;} -->
<h5 class="sectionedit16" id="on-and-off-screenpresentation-and-notation-in-interactive-electronic-music">On- and off-screen: presentation and notation in interactive electronic music</h5>
<div class="level5">

<p>
<strong>Pete Furniss</strong> <em>Edinburgh College of Art - University of Edinburgh</em>
</p>

<p>
<a href="../../lib/exe/detail.php/evenements/2014-06-30-workshop/furnissc5ca.html?id=evenements%3A2014-06-30-nimew" class="media" title="evenements:2014-06-30-workshop:furniss.png"><img src="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/furnissa6aa.png_%3b?w=200&amp;tok=0b606f" class="mediaright" alt="" width="200" /></a>At April 2014 performance at the Dialogues Festival in Edinburgh, I presented three works for clarinet and computer (one written, two improvised), each engendering a sense of interactive play in both performer and audience. Here I examine the communication of the material for each piece, as it was presented by the composer: firstly by a combination of text description, on-screen prompts and <abbr title="Graphical User Interface">GUI</abbr>; secondly by traditional notation (with the aid of a guide stave, cue numbers and sound file for testing); finally, by stand-alone application. In each case I was fortunate to work in close personal contact with the composer, which afforded the significant benefit of discourse and contextualisation.
</p>

<p>
It has been observed (Berweck, 2012, Dudas &amp; Furniss, 2014) that software interfaces for live electronic performance have tended, on the whole, to be designed for use by either the composer or a technical specialist. An increasing number of musicians are choosing to play with full, onstage control of electronic elements (where practical), leading to the need for a more nuanced approach, towards “expressive, higher-order music notations” (Stowell &amp; McLean, 2013). Using example images, audio and demonstration at the clarinet, the presentation will deal with the requirements of a performer in managing synchronicity, audio input/output, trust, attention, peripheral vision and cueing within an interactive environment.
Andrew May’s Ripped Up Maps (Fig.1) is an interactive improvisation for any instrument and computer. It has a ‘score’ of musical and technical text instructions, and an on-screen graphical interface in Max/MSP. Here, a user-adapted ‘presentation mode’ in Max 6 provides enhanced visual feedback, clearly registered from a distance of 1.5-2m from the laptop screen, achieved with blocks of distinct colour and large format graphics on an anti-glare background. On-screen résumés of both software and score instructions are optionally available in the display.
Again employing a personally adapted <abbr title="Graphical User Interface">GUI</abbr>, the interface for Richard Dudas’ Prelude No.1 for clarinet and computer (Fig.2) clarifies the process of setting up audio and software, and provides clear and relevant visual feedback. A traditionally notated score is used here, with numbered cues, some of which were repositioned to provide accuracy and reassurance.
Martin Parker’s gruntCount (Fig. 3) is presented as a stand-alone application, with a graphical interface showing various plotted journeys through pre-selected sound processing modules, as well as software and audio settings and a non-linear timeline, controlled by the sound of the instrument at a set threshold. There are several versions of the piece built into the software, to which further adjustments can be made. Performers may also create new plots, effectively providing the potential to expand the app into a tool for collaborative composition.
</p>

<p>
Links:
</p>
<ul>
<li class="level1"><div class="li"> <a href="http://www.petefurniss.com/" class="urlextern" title="http://www.petefurniss.com" rel="nofollow">http://www.petefurniss.com</a> (mp3 player has examples of work by Martin Parker and Richard Dudas)</div>
</li>
<li class="level1"><div class="li"> Blog, and outline of my PhD research (begun September 2013): <a href="http://www.petefurniss.wordpress.com/" class="urlextern" title="http://www.petefurniss.wordpress.com" rel="nofollow">http://www.petefurniss.wordpress.com</a></div>
</li>
<li class="level1"><div class="li"> Link to the performance referred to in the presentation <a href="http://digital.eca.ed.ac.uk/events/late-lab-featuring-jean-francois-laporte-jess-aslan-and-pete-furniss-18th-april-2014/" class="urlextern" title="http://digital.eca.ed.ac.uk/events/late-lab-featuring-jean-francois-laporte-jess-aslan-and-pete-furniss-18th-april-2014/" rel="nofollow">http://digital.eca.ed.ac.uk/events/late-lab-featuring-jean-francois-laporte-jess-aslan-and-pete-furniss-18th-april-2014/</a></div>
</li>
<li class="level1"><div class="li"> Martin Parker, gruntCount CD (listen) <a href="http://www.tinpark.com/2013/11/gruntcountcddigital/" class="urlextern" title="http://www.tinpark.com/2013/11/gruntcountcddigital/" rel="nofollow">http://www.tinpark.com/2013/11/gruntcountcddigital/</a></div>
</li>
<li class="level1"><div class="li"> Andrew May <a href="http://cemi.music.unt.edu/may/" class="urlextern" title="http://cemi.music.unt.edu/may/" rel="nofollow">http://cemi.music.unt.edu/may/</a></div>
</li>
<li class="level1"><div class="li"> Richard Dudas <a href="http://www.richarddudas.com/" class="urlextern" title="http://www.richarddudas.com" rel="nofollow">http://www.richarddudas.com</a></div>
</li>
<li class="level1"><div class="li"> <a href="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/pete-furniss.pdf_%3b" class="media mediafile mf_pdf" title="evenements:2014-06-30-workshop:pete-furniss.pdf (9 MB)">Slides</a></div>
</li>
<li class="level1"><div class="li"> <a href="https://www.youtube.com/watch?v=6cSKcRyv5Fw" class="urlextern" title="https://www.youtube.com/watch?v=6cSKcRyv5Fw" rel="nofollow">Video</a></div>
</li>
</ul>
<hr />

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;On- and off-screen: presentation and notation in interactive electronic music&quot;,&quot;hid&quot;:&quot;on-and-off-screenpresentation-and-notation-in-interactive-electronic-music&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:16,&quot;range&quot;:&quot;18041-21877&quot;} -->
<h5 class="sectionedit17" id="john-cage-solo-for-sliding-trombone-a-computer-assisted-performance-approach">John Cage Solo for Sliding Trombone, a Computer Assisted Performance approach</h5>
<div class="level5">

<p>
<strong>Benny Sluchin, Mikhail Malt</strong> <em>IRCAM</em>
</p>

<p>
<a href="../../lib/exe/detail.php/evenements/2014-06-30-workshop/sluchinc5ca.html?id=evenements%3A2014-06-30-nimew" class="media" title="evenements:2014-06-30-workshop:sluchin.png"><img src="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/sluchin618a.png_%3b?w=200&amp;tok=57b22a" class="mediaright" alt="" width="200" /></a>John Cage has left an abundant and varied work, which stays enigmatic, contested and not entirely understood. During the 1950s, in order to define the process of his thinking, he employs three different terms: indeterminacy, unintentionally and chance. The first two terms will be used for an unintended result of a performance. A computer interface is demonstrated to assist in a performance, where theses notions are the essence.<br/>

The Concert for Piano and Orchestra (1957-58) is one of the most significant works of Cage. The instrumental parts of the “orchestra”, as composed of isolated events that are to be put together “according to a program” and to follow the composer’s instructions, are especially difficult of approach.<br/>

The presentation will focus on the trombone part of the Concert using a CAP (Computer Assisted Performance) interface that follows the composer’s concepts and the structure of the work. The performance of two short examples will be included.
Each instrument page is a collection of punctual musical events, (see Figure 1), displayed on 12 music sheets. Each event is a compound one, having relative pitch, dynamics, playing modes and other indications. The events are distributed variously on each sheet, from extreme density (music system with around 10 events) to empty staves.
Analyzing the Cage instructions and his musical and aesthetical points of view, we realize that the traditional scores make the players performance difficult, especially regarding the “unintentional” choices of different musical objects. The material and sequential aspect of paper scores is an obstacle to the realization of the Cage’s main idea that the player could go, freely, without constraint and without intention through the score.
Three kinds of ways displaying music will be shown. A further possibility to organize the events according to logical criteria is available. The memorization of a “version” is also a way to test and analyze one of the infinite possible realizations of the work.
Links:
</p>
<ul>
<li class="level1"><div class="li"> <a href="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/sluchin_malt_nime2014_.pdf_%3b" class="media mediafile mf_pdf" title="evenements:2014-06-30-workshop:sluchin_malt_nime2014_.pdf (3.8 MB)">Slides</a></div>
</li>
</ul>
<hr />

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;John Cage Solo for Sliding Trombone, a Computer Assisted Performance approach&quot;,&quot;hid&quot;:&quot;john-cage-solo-for-sliding-trombone-a-computer-assisted-performance-approach&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:17,&quot;range&quot;:&quot;21878-24187&quot;} -->
<h5 class="sectionedit18" id="deriving-a-chart-organised-notation-from-a-sonogram-based-explorationtiaals-tools-for-interactive-aural-analysis">Deriving a Chart-Organised Notation from a Sonogram Based Exploration: TIAALS (Tools for Interactive Aural Analysis)</h5>
<div class="level5">

<p>
<strong>Michael Clarke (1), Frédéric Dufeu (1), Peter Manning (2)</strong> <em>CeReNeM, University of Huddersfield - Durham University</em>
</p>

<p>
<a href="../../lib/exe/detail.php/evenements/2014-06-30-workshop/clarkec5ca.png?id=evenements%3A2014-06-30-nimew" class="media" title="evenements:2014-06-30-workshop:clarke.png"><img src="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/clarke1c41.png_%3b?w=200&amp;tok=e61509" class="mediaright" alt="" width="200" /></a>The TaCEM project (Technology and Creativity in Electroacoustic Music), funded by the AHRC for 30 months (2012-2015), investigates eight case studies from the electroacoustic repertoire. Additionally, a generic software environment is being developed: TIAALS (Tools for Interactive Aural Analysis).
Traditional modes of notation often prove limited for representing electroacoustic music and its structural elements. TIAALS enables its users to build interactive aural analyses from any recorded work. Like Sonic Visualiser or EAnalysis, TIAALS builds a sonogram of the considered work. This facilitates a fully interactive exploration of the piece under a time-frequency representation, as well as the graphical creation of interactive sound objects and structures (figure 1).<br/>

One important specificity of TIAALS is to provide access to a chart maker, upon which the user can arrange objects extracted from the sonogram representation, hence enabling a form of notation for the relevant sound objects and structures that is independent from their original time and frequency bounds (figure 2). Objects represented on the chart are fully interactive, so that the user can elaborate and present his representation both visually and aurally in both the sonogram and the chart maker views.<br/>

Developed in Cycling’74’s Max, TIAALS can be well integrated to other pieces of software developed for specific case studies, and its future releases will expand on relevant representation features. This proposal aims at presenting, in the context of the workshop, the current beta version of TIAALS and engaging a discussion for its further perspectives regarding the notation of music that exists primarily in sound rather than on the score.
</p>

<p>
Links:
</p>
<ul>
<li class="level1"><div class="li"> TaCEM project web page / TIAALS beta version download link: <a href="http://www.hud.ac.uk/research/researchcentres/tacem/" class="urlextern" title="http://www.hud.ac.uk/research/researchcentres/tacem/" rel="nofollow">http://www.hud.ac.uk/research/researchcentres/tacem/</a></div>
</li>
<li class="level1"><div class="li"> Interactive Aural Analysis: <a href="http://www.hud.ac.uk/research/researchcentres/iaa/" class="urlextern" title="http://www.hud.ac.uk/research/researchcentres/iaa/" rel="nofollow">http://www.hud.ac.uk/research/researchcentres/iaa/</a></div>
</li>
<li class="level1"><div class="li"> Full ICMC2013 paper on TaCEM and TIAALS: <a href="http://eprints.hud.ac.uk/18726/1/icmc2013_clarke_dufeu_manning.pdf" class="urlextern" title="http://eprints.hud.ac.uk/18726/1/icmc2013_clarke_dufeu_manning.pdf" rel="nofollow">http://eprints.hud.ac.uk/18726/1/icmc2013_clarke_dufeu_manning.pdf</a></div>
</li>
<li class="level1"><div class="li"> <a href="https://www.youtube.com/watch?v=CP8Ocb2j1eY" class="urlextern" title="https://www.youtube.com/watch?v=CP8Ocb2j1eY" rel="nofollow">Video</a></div>
</li>
</ul>
<hr />

<p>
<a href="http://www.afim-asso.org/" class="media" title="http://www.afim-asso.org" rel="nofollow"><img src="../../lib/exe/fetch.php/logos/afim-logo-bleu-110.gif_%3b" class="media" title="AFIM" alt="AFIM" /></a> <a href="http://inedit.ircam.fr/" class="media" title="http://inedit.ircam.fr/" rel="nofollow"><img src="../../lib/exe/fetch.php/logos/logo_inedit_90.png_%3b" class="media" title="INEDIT" alt="INEDIT" /></a> <a href="http://www.agence-nationale-recherche.fr/" class="media" title="http://www.agence-nationale-recherche.fr/" rel="nofollow"><img src="../../lib/exe/fetch.php/logos/anr07-80.gif_%3b" class="media" title="ANR" alt="ANR" /></a> <a href="http://repmus.ircam.fr/efficace/" class="media" title="http://repmus.ircam.fr/efficace/" rel="nofollow"><img src="../../lib/exe/fetch.php/logos/efficacec291.png_%3b?w=120&amp;tok=9df425" class="media" title="EFFICAC(e)" alt="EFFICAC(e)" width="120" /></a> <a href="http://www.nime2014.org/" class="media" title="http://www.nime2014.org/" rel="nofollow"><img src="../../lib/exe/fetch.php/evenements/2014-06-30-workshop/nime14ae3b.png_%3b?w=70&amp;tok=51c132" class="media" title="NIME2014" alt="NIME2014" width="70" /></a>
</p>

</div>
<!-- EDIT{&quot;target&quot;:&quot;section&quot;,&quot;name&quot;:&quot;Deriving a Chart-Organised Notation from a Sonogram Based Exploration: TIAALS (Tools for Interactive Aural Analysis)&quot;,&quot;hid&quot;:&quot;deriving-a-chart-organised-notation-from-a-sonogram-based-explorationtiaals-tools-for-interactive-aural-analysis&quot;,&quot;codeblockOffset&quot;:0,&quot;secid&quot;:18,&quot;range&quot;:&quot;24188-&quot;} --></div>
</body>

<!-- Mirrored from notation.afim-asso.org/doku.php/evenements/2014-06-30-nimew?do=export_xhtml by HTTrack Website Copier/3.x [XR&CO'2014], Mon, 19 Oct 2020 07:56:30 GMT -->
</html>
